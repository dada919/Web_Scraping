{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data with python \n",
    "\n",
    "\n",
    "According to Wikipedia Web scraping is data scraping used for extracting data from websites. Web scraping software may directly access the World Wide Web using the Hypertext Transfer Protocol or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis. \n",
    "\n",
    "<center>\n",
    "\n",
    "![](https://kinsta.com/wp-content/uploads/2022/07/Web-scraping.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "## Goals 🚀\n",
    "\n",
    "In order to be a real scraping ninja we will learn in this course :\n",
    "\n",
    "- What is scraping and how DOM web data are organised \n",
    "- What is an HTTP request and handle them with python \n",
    "- What is concurrency, parallelism, threading and multiprocessing programming \n",
    "- Leverage asynchronous programming in python \n",
    "- Leverage proxies and headers rotating   \n",
    "- Handle the principal python libraries for scraping : BeautifulSoup, Selenium and Scrapy \n",
    "\n",
    "\n",
    "Let's begin with understanding simple requests with the `requests` library of python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making `requests`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple request on webpage\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html>\\n    <head>\\n        <title>A simple example page</title>\\n    </head>\\n    <body>\\n        <p>Here is some simple content for this page.</p>\\n    </body>\\n</html>'\n"
     ]
    }
   ],
   "source": [
    "print(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<!DOCTYPE html>',\n",
       " '<html>',\n",
       " '    <head>',\n",
       " '        <title>A simple example page</title>',\n",
       " '    </head>',\n",
       " '    <body>',\n",
       " '        <p>Here is some simple content for this page.</p>',\n",
       " '    </body>',\n",
       " '</html>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formating \n",
    "page.text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may saw it is not the perfect return format... We will see how to leverage the tags and extract the text directly. For this we'll be using `beautifulsoup` library, an indispensable parser and tag browser for larger pages.\n",
    "\n",
    "First let's view how to handle data with python if you are not familiar with this langage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling overview \n",
    "\n",
    "Data wrangling, also known as data munging or data handling, is a crucial step in the data preparation process, and it plays a significant role in web scraping. In order to master web scraping you will have to : \n",
    "\n",
    "1. Cleaning and structuring Raw Data\n",
    "2. Data Transformation : aggregation and summarization\n",
    "3. Integration with other Data sources\n",
    "4. Preparing for analysis and visualization\n",
    "\n",
    "First thing thirst, let's see how to manage basics files like `json`, `csv`, `txt`, and `xml` files. \n",
    "\n",
    "\n",
    "### Manage static files in python \n",
    "\n",
    "Knowing how to manage files in Python is crucial before diving deeper into web scraping for saving scraped data in various formats. Without the ability to efficiently write to and read from files, it would be challenging to handle the data collected via scraping.\n",
    "\n",
    "Let's begin by reading and writing in a `txt`file here : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new file.\n"
     ]
    }
   ],
   "source": [
    "# Attempt to read from a file\n",
    "try:\n",
    "    with open('example.txt', 'r') as file:\n",
    "        content = file.read()\n",
    "        print(content)\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create it and write a default message\n",
    "    with open('example.txt', 'w') as file:\n",
    "        file.write(\"This is a new file.\")\n",
    "        print(\"File 'example.txt' was not found and has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new file.\n"
     ]
    }
   ],
   "source": [
    "with open('example.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the native `os` library in order to navigate into our file system like this : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "app\n",
      ".env\n",
      "Jenkinsfile\n",
      "docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#list all files in a directory\n",
    "for file in os.listdir('/Users/mac/workspace/ds_course/micro-services/jenkins'):\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `utf-8` Encoding \n",
    "\n",
    "When you talk about files you can not ignore understanding and handling encodings with a simple `encoding` parameter. If you want to know more about encoding you can take a look to this wikipedia page [here](https://en.wikipedia.org/wiki/Unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new file.\n"
     ]
    }
   ],
   "source": [
    "with open('example.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and writing `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"name\": \"Alice Brown\",\n",
    "        \"department\": \"Marketing\",\n",
    "        \"salary\": 70000\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Bob Smith\",\n",
    "        \"department\": \"Sales\",\n",
    "        \"salary\": 65000\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Carol Jones\",\n",
    "        \"department\": \"IT\",\n",
    "        \"salary\": 75000\n",
    "    }\n",
    "]\n",
    "\n",
    "#write this variable inside a json file\n",
    "with open('output.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Alice Brown', 'department': 'Marketing', 'salary': 70000}, {'name': 'Bob Smith', 'department': 'Sales', 'salary': 65000}, {'name': 'Carol Jones', 'department': 'IT', 'salary': 75000}]\n"
     ]
    }
   ],
   "source": [
    "#then read the data\n",
    "with open('output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator \n",
    "\n",
    "Generators are a type of iterable, like lists or tuples, but they generate items on-the-fly and don't store them in memory. This makes them more memory-efficient for large datasets. We use the `yield` keyword in order to call them.\n",
    "\n",
    "The efficiency of the generator approach is noticeable if the file is large, as it processes one line at a time without loading the entire file into memory. The traditional approach may be slower due to the overhead of loading the entire file, let's see that ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import csv\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n",
      "Traditional approach took: 0.03126811981201172 seconds\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def csv_reader(file_content):\n",
    "    return csv.reader(StringIO(file_content))\n",
    "\n",
    "# Fetch the file content from the URL\n",
    "url = 'https://gist.githubusercontent.com/bdallard/d4a3e247e8a739a329fd518c0860f8a8/raw/82fb43adc5ce022797a5df21eb06dd8e755145ea/data-json.csv'\n",
    "response = requests.get(url)\n",
    "file_content = response.text\n",
    "\n",
    "tmp=0\n",
    "start_time = time.time()\n",
    "csv_data = csv_reader(file_content)\n",
    "for row in csv_data:\n",
    "    tmp+=int(row[0][-1]) #some dummy operation\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Traditional approach took:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "Generator approach took: 0.04893088340759277 seconds\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def csv_reader_gen(file_content):\n",
    "    for row in csv.reader(StringIO(file_content)):\n",
    "        yield row\n",
    "\n",
    "# Fetch the file content from the URL\n",
    "url = \"https://gist.githubusercontent.com/bdallard/d4a3e247e8a739a329fd518c0860f8a8/raw/82fb43adc5ce022797a5df21eb06dd8e755145ea/data-json.csv\"\n",
    "response = requests.get(url)\n",
    "file_content = response.text\n",
    "\n",
    "tmp=0\n",
    "start_time = time.time()\n",
    "csv_gen = csv_reader_gen(file_content)\n",
    "for row in csv_gen:\n",
    "    tmp+=int(row[0][-1]) #some dummy operation\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Generator approach took:\", end_time - start_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP in a nutshell\n",
    "\n",
    "HTTP stands for HyperText Transfer Protocol. It's the foundation of data communication on the World Wide Web. Essentially, it's a protocol used for transmitting data over a network. Most of the information that you receive through your web browser is delivered via HTTP.\n",
    "\n",
    "**An HTTP request is a message sent by a client (like a web browser or a mobile app) to a server to request a specific action.** This action can be fetching a web page, submitting form data, downloading a file, etc.\n",
    "\n",
    "<center>\n",
    "\n",
    "![](https://documentation.help/DogeTool-HTTP-Requests-vt/http_requestmessageexample.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "As you can see on the schema above an HTTP request is formed with : \n",
    "\n",
    "- Method: Indicates what type of action you're requesting. Common methods include GET (retrieve data), POST (submit data), PUT (update data), and DELETE (remove data).\n",
    "- URL (Uniform Resource Locator): Specifies the location of the resource (like a web page or an image) on the server.\n",
    "- Headers: Provide additional information (like the type of browser making the request, types of response formats that are acceptable, etc.).\n",
    "- Body: Contains data sent to the server. This is typically used with POST and PUT requests.\n",
    "\n",
    "**How does it work ?**\n",
    "\n",
    "When you type a URL into your browser and press Enter, your browser sends an HTTP GET request to the server that hosts that URL.\n",
    "The server processes the request, and if everything goes well, it sends back a response. This response usually contains the HTML content of the web page you requested.\n",
    "\n",
    "The response from the server includes a status code (like 200 for a successful request, 404 for not found, etc.), headers (similar to request headers but providing information from the server), and usually, a body (which contains the requested data, if any). See http codes on [wikipedia](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). \n",
    "\n",
    "> HTTP is a stateless protocol, meaning each request-response pair is independent. Servers don't retain information about previous interactions. Techniques like cookies are used to \"remember\" state across requests.\n",
    "\n",
    "#### Secure HTTP - HTTPS\n",
    "\n",
    "When security is a concern, HTTPS (HTTP Secure) is used. It encrypts the request and response, protecting the data from being read or tampered with by intermediaries. See more details about basic certificates into de docker https section.\n",
    "\n",
    "#### Example with the `request` python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://httpbin.org/ip') \n",
    "#print(response.json()['origin']) #your personnal ip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand proxies \n",
    "\n",
    "A proxy server is a server application that acts as an intermediary between a client requesting a resource and the server providing that resource, more infos on [wikipedia](https://en.wikipedia.org/wiki/Proxy_server)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Proxy_concept_fr.svg/752px-Proxy_concept_fr.svg.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install free proxy tool from : https://github.com/jundymek/free-proxy\n",
    "#!pip install free-proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find free proxies here : https://free-proxy-list.net \n",
    "\n",
    "Or more pro solutions for goods tool like Captia bypass here : https://www.zenrows.com/solutions/bypass-captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fp.fp import FreeProxy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://103.127.1.130:80'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy = FreeProxy(country_id=['FR']).get(); proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://139.59.1.14:8080', 'http://20.24.43.214:80', 'http://20.24.43.214:80']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_list = [FreeProxy(country_id=['FR']).get() for x in range(3)]; proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.24.43.214\n"
     ]
    }
   ],
   "source": [
    "proxies = {'http': proxy_list[1]} \n",
    "response = requests.get('http://httpbin.org/ip', proxies=proxies) \n",
    "print(response.json()['origin']) # our proxy !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we have now a different IP address, at least the server detect an other ip and not our public router IP 🧙🏼‍♂️\n",
    "\n",
    "### Headers \n",
    "\n",
    "Now let's get deeper a little with our request header in order to fool our target with `User-Agent` (abbreviated as UA). A user agent is a string that a web browser sends to a web server identifying itself. This string contains details about the browser type, rendering engine, operating system, and sometimes device type. In web scraping, the user agent plays a crucial role for several reasons:\n",
    "\n",
    "- Browser Identification: The user agent tells the server what kind of browser is making the request. Different browsers may support different features or render web pages differently.\n",
    "- Device and OS Identification: The user agent can also indicate the operating system and the device (desktop, mobile, tablet, etc.), which can affect how web content is delivered.\n",
    "\n",
    "Let's see a basic example of the informations the target site will get if we use Python Requests or cURL without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.31.0', 'X-Amzn-Trace-Id': 'Root=1-65733af4-7e6abbd77660881e470ae532'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://httpbin.org/headers') \n",
    "print(response.json()['headers'])\n",
    "# python-requests/2.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"headers\": {\r\n",
      "    \"Accept\": \"*/*\", \r\n",
      "    \"Host\": \"httpbin.org\", \r\n",
      "    \"User-Agent\": \"curl/7.64.1\", \r\n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-65733af5-2f7996e30facb6690797b21e\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl http://httpbin.org/headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "#try a custom user-agent\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"} \n",
    "response = requests.get('http://httpbin.org/headers', headers=headers) \n",
    "print(response.json()['headers']['User-Agent']) # Mozilla/5.0 ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (iPad; CPU OS 13_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Mobile/15E148 Safari/604.1\n"
     ]
    }
   ],
   "source": [
    "#more user-agent, thanks chatgpt 🤓\n",
    "import random\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (iPad; CPU OS 13_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/604.1.34 (KHTML, like Gecko) Edge/90.0.818.56',\n",
    "    'Mozilla/5.0 (Linux; Android 10; SM-A505FN) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Linux; Android 11; Pixel 3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0'\n",
    "]\n",
    "\n",
    "user_agent = random.choice(user_agents) \n",
    "headers = {'User-Agent': user_agent} \n",
    "response = requests.get('https://httpbin.org/headers', headers=headers) \n",
    "print(response.json()['headers']['User-Agent']) \n",
    "# Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a closer look to our request [here](http://httpbin.org/headers) you will see the entier header look like this :\n",
    "\n",
    "```bash\n",
    "{\n",
    "  \"headers\": {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate\", \n",
    "    \"Accept-Language\": \"fr-fr\", \n",
    "    \"Host\": \"httpbin.org\", \n",
    "    \"Upgrade-Insecure-Requests\": \"1\", \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6.1 Safari/605.1.15\", \n",
    "    \"X-Amzn-Trace-Id\": \"Root=1-6572fed4-5a7b863b4842def83f9030c4\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'en-GB,en;q=0.5', 'Host': 'httpbin.org', 'Sec-Fetch-Dest': 'document', 'Sec-Fetch-Mode': 'navigate', 'Sec-Fetch-Site': 'none', 'Sec-Fetch-User': '?1', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0', 'X-Amzn-Trace-Id': 'Root=1-65733af9-0e45d0f96abddec2158e071e'}\n"
     ]
    }
   ],
   "source": [
    "headers_list = [\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Ch-Ua\": \"\\\"Chromium\\\";v=\\\"92\\\", \\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"92\\\"\",\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    },\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0\"\n",
    "    },\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "    },\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-GB,en;q=0.5\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0\"\n",
    "    },\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Safari/605.1.15\"\n",
    "    }\n",
    "]\n",
    "\n",
    "headers = random.choice(headers_list) \n",
    "response = requests.get('https://httpbin.org/headers', headers=headers, proxies=proxies) \n",
    "print(response.json()['headers']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency and parallelism \n",
    "\n",
    "![](https://media.licdn.com/dms/image/D4D12AQGM9V1nzAH7Lg/article-cover_image-shrink_600_2000/0/1696261947807?e=2147483647&v=beta&t=nRfLCT6JEt15eGwWgh711IBfQWD9HQ3U-3xmEl8UpQQ)\n",
    "\n",
    "### Concurrency\n",
    "\n",
    "Concurrency is about **dealing with multiple tasks at the same time. It doesn't necessarily mean these tasks are executed simultaneously.** It's more about the structure of the program, where tasks are broken into smaller, independent units which are processed in overlapping time periods.\n",
    "\n",
    "#### Concurrency use case \n",
    "\n",
    "Web scraping involves fetching data from multiple web pages simultaneously. By employing concurrency, you can make concurrent requests to different web pages using threads or asynchronous programming libraries like asyncio or gevent. This allows for efficient utilization of I/O resources and faster data retrieval.\n",
    "\n",
    "### Parallelism \n",
    "\n",
    "Parallelism in the contrary is about doing multiple tasks simultaneously, often requiring multiple processors or cores. It's about executing multiple operations at the exact same time.\n",
    "\n",
    "#### Parallelism use case \n",
    "\n",
    "When dealing with large datasets, parallelism can significantly enhance data processing speed. For instance, if you need to perform complex calculations on each data point independently, you can distribute the workload across multiple CPU cores using multiprocessing. Each core can handle a portion of the data, leading to faster overall processing time.\n",
    "\n",
    "### Threading and Multiprocessing \n",
    "\n",
    "#### Threading\n",
    "\n",
    "Threading in Python allows for concurrent execution of tasks by utilizing multiple threads within a single process. Threads share the same memory space and can switch rapidly between tasks, giving the illusion of parallel execution. However, due to the Global Interpreter Lock (GIL) in Python, threading is more suitable for I/O-bound tasks, where threads can wait for I/O operations without blocking the entire process. This makes threading well-suited for achieving concurrency in Python applications.\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "#define a function that must be executed using threads\n",
    "def thread_function(name):\n",
    "    print(\"Hello from thread\", name)\n",
    "\n",
    "#create a thread and execute the function\n",
    "thread = threading.Thread(target=thread_function, args=(\"Thread 1\",)) #create a thread taking our desired function as arguement\n",
    "thread.start()\n",
    "thread.join() #thread.start() starts the thread and thread.join() stops the thread\n",
    "```\n",
    "\n",
    "\n",
    "### Multiprocessing\n",
    "\n",
    "Multiprocessing in Python enables true parallelism by utilizing multiple processes that can run on separate CPU cores. Each process has its own memory space, allowing for independent execution of tasks. \n",
    "\n",
    "Multiprocessing is ideal for CPU-bound tasks, where the workload can be divided and executed in parallel across multiple cores. Unlike threading, multiprocessing can fully utilize multiple CPU cores and achieve significant speed improvements for parallel tasks.\n",
    "\n",
    "```python\n",
    "import multiprocessing module\n",
    "import multiprocessing\n",
    "#define the function that must be executed parallely\n",
    "def process_function(name):\n",
    "    print(\"Hello from process\", name)\n",
    "#start the process using multiprocessing.Process() method, it takes function name as arguement\n",
    "process = multiprocessing.Process(target=process_function,args=(\"process1\",))\n",
    "process.start() #process.start() starts the process and process.join() stops the process\n",
    "process.join()\n",
    "```\n",
    "\n",
    "More example on the [geeksforgeeks website](https://www.geeksforgeeks.org/difference-between-multithreading-vs-multiprocessing-in-python/) 🤖\n",
    "\n",
    "### Example in python \n",
    "\n",
    "Let's code a little example and evaluate the time it takes to download some images using three different methods in Python: a standard approach, threading, and multiprocessing. \n",
    "\n",
    "For this purpose, we'll employ the `threading` and `multiprocessing` modules to implement the respective methods, and the `timeit` module will be used for timing the processes.\n",
    "\n",
    "Our objective is to highlight the performance variation among these techniques. By recording how long it takes to download several images with each technique, we'll be able to see how concurrency and parallelism affect the total time required for execution.\n",
    "\n",
    "Because we are smart coders we will download very little images (1px,1px) from this site : [https://picsum.photos/1](https://picsum.photos/1) 🤓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import requests\n",
    "import threading\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_urls = []\n",
    "for i in range(0,100):\n",
    "    image_urls.append(f'https://picsum.photos/{i}')\n",
    "\n",
    "len(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(f\"images/{image_url.split('/')[-1]}.png\", 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f'Error downloading image {image_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Execution Time for 100 images: 26.95955391699954\n"
     ]
    }
   ],
   "source": [
    "def normal_execution():\n",
    "    start = timeit.default_timer()\n",
    "    for image_url in image_urls:\n",
    "        download_image(image_url)\n",
    "    end = timeit.default_timer()\n",
    "    print(f'Normal Execution Time for {len(image_urls)} images: {end-start}')\n",
    "\n",
    "normal_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threading Execution Time for 100 images: 13.221584853999957\n"
     ]
    }
   ],
   "source": [
    "def threading_download():\n",
    "    start = timeit.default_timer()\n",
    "    threads = []\n",
    "\n",
    "    for image_url in image_urls:\n",
    "        t = threading.Thread(target=download_image,args=(image_url,))\n",
    "        threads.append(t)\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    end = timeit.default_timer()\n",
    "\n",
    "    print(f'Threading Execution Time for {len(image_urls)} images: {end-start}')\n",
    "\n",
    "threading_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiprocessing Execution Time for 100 images:: 4.486782826000308\n"
     ]
    }
   ],
   "source": [
    "def multiprocessing_download():\n",
    "    start = timeit.default_timer()\n",
    "    processes = []\n",
    "\n",
    "    for image_url in image_urls:\n",
    "        #print('downloading image ',image_url)\n",
    "        p = multiprocessing.Process(target=download_image,args=(image_url,))\n",
    "        processes.append(p)\n",
    "    \n",
    "    for process in processes:\n",
    "        process.start()\n",
    "    \n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    end = timeit.default_timer()\n",
    "\n",
    "    print(f'Multiprocessing Execution Time for {len(image_urls)} images:: {end-start}')\n",
    "\n",
    "multiprocessing_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Asynchronous Programming\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:581/1*frETB534XGOkTbXgAcctiA.png)\n",
    "\n",
    "**Asynchronous programming is a type of parallel programming in which a unit of work is allowed to run separately from the primary application thread. When the work is complete, it notifies the main thread about completion or failure of the worker thread.** \n",
    "\n",
    "There are numerous benefits to using it, such as improved application performance and enhanced responsiveness but we will not go into pythonic details here.\n",
    "\n",
    "More informations : \n",
    "\n",
    "- [Async IO official doc](https://docs.python.org/3/library/asyncio.html)\n",
    "- [AIOHTTP official doc](https://docs.aiohttp.org/en/stable/)\n",
    "- [Async IO in Python: A Complete Walkthrough](https://realpython.com/async-io-python/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
