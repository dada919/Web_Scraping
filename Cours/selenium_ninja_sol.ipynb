{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap like a Ninja\n",
    "\n",
    "In order to be a real ninja scraper, you will have to build a custom selenium driver üòé\n",
    "\n",
    "Writing a custom Selenium driver offers several benefits, particularly when dealing with dynamic and complex web pages that may have measures to detect and block automated scraping. It provides a level of control and customization that is necessary for effective scraping of modern web applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our goal üöÄ\n",
    "\n",
    "*__Marche √† suivre__*\n",
    "\n",
    "L'URL √† utiliser pour cet exercice est :\n",
    "https://www.welcometothejungle.com/fr/jobs?page=1&refinementList%5Bprofession_name.fr.Tech%5D%5B%5D=Data%20Analysis&refinementList%5Bcontract_type_names.fr%5D%5B%5D=CDI\n",
    "\n",
    "On peut remarquer que :\n",
    "\n",
    "1. Il y a plusieurs pages de r√©sultats, que l'on peut parcourir simplement **en changeant `page=k` dans l'URL**. Il y a **30 postes propos√©s par page** de r√©sultats.\n",
    "<br><br>\n",
    "2. Welcome to the jungle a impl√©ment√© des mesures anti-scraping. En particulier, une partie du HTML est cach√©e lorsque l'on requ√™te la page avec  `requests`. Il est indispensable de **commencer √† scroller** la page pour lancer le code JavaScript qui r√©v√®le le contenu cach√©.\n",
    "<br><br>\n",
    "$\\rightarrow$ Pour r√©soudre ce probl√®me, on ne peut se contenter de BeautifulSoup. Il faut **Simuler le comportement d'une vraie personne** qui parcourt la page avec sa souris, c'est donc **Selenium** qu'il nous faut\n",
    "<br><br>\n",
    "$\\rightarrow$ Voil√† une fonction qui permet de simuler un scroll de page jusqu'√† la i√®me offre d'emploi :\n",
    "<br><br>\n",
    "\n",
    "```python\n",
    "def scroll(driver, i):\n",
    "        scroll_delta = int(250)\n",
    "        scroll_delta += 140*i\n",
    "        driver.execute_script(\"window.scrollBy(0, \"+ str(scroll_delta) + \")\")\n",
    "```\n",
    "<br>\n",
    "\n",
    "3. Une autre mesure anti-scraping concerne les noms de classes, les ids et m√™me les liens vers des images dans le code HTML. Tous ces noms sont al√©atoires (ex:`class=\"sc-1flb27e-5 cdtiMs\"`) et changent √† chaque chargement de la page.<br><br>\n",
    "$\\rightarrow$ Une bonne nouvelle quand m√™me : toutes les classes ne sont pas al√©atoires, certaines restent fixes. Pour les noms al√©atoires, certaines lettres du nom sont fixes √©galement. On peut donc toujours utiliser des similarit√©s pour d√©signer certains tags sp√©cifiques (ex : le tag header, contenant le nombre total de r√©sultats, commence toujours par \"hd\").<br><br>\n",
    "$\\rightarrow$ Pour exploiter cette faille, il est conseill√© d'utiliser la m√©thode Selenium **`find_elements_by_css_selector()`** pour d√©signer des tags pr√©cis, car cette m√©thode permet de d'identifier un tag par un texte partiel (ex: `driver.find_elements_by_css_selector(\"header[class^='hd']\")` pour toutes les classes de headers qui commencent par \"hd\").\n",
    "<br><br>\n",
    "4. Au bout du compte, on souhaite sauvegarder le contenu de chaque offre d'emploi dans un fichier .txt.<br><br>\n",
    "$\\rightarrow$ Il va donc falloir cliquer sur chaque offre d'emploi avec la m√©thode **`.click()`** de Selenium. Pour chaque offre d'emploi, le contenu de l'offre est stock√© dans un dictionnaire √† l'int√©rieur d'un tag `<script>`. On peut utiliser la m√©thode  **`json.loads()`** pour manipuler ce dictionnaire. On peut finalement l'enregistrer en .txt avec les fonctions **`open()`** et **`.write`**.\n",
    "5. Sauvegarder le contenu de chaque offre d'emploi dans une database postgres puis mongodb. <br><br>\n",
    "$\\rightarrow$Utiliser un `dataframe` comme structure interm√©diaire. <br>\n",
    "$\\rightarrow$Quel est le probl√®me de postgres? <br>\n",
    "$\\rightarrow$Quel est la diff√©rence avec mongodb?\n",
    "\n",
    "## Understand proxies \n",
    "\n",
    "A proxy server is a server application that acts as an intermediary between a client requesting a resource and the server providing that resource, more infos on [wikipedia](https://en.wikipedia.org/wiki/Proxy_server)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Proxy_concept_fr.svg/752px-Proxy_concept_fr.svg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install free proxy tool from : https://github.com/jundymek/free-proxy\n",
    "#!pip install free-proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find free proxies here : https://free-proxy-list.net \n",
    "\n",
    "Or more pro solutions for goods tool like Captia bypass here : https://www.zenrows.com/solutions/bypass-captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fp.fp import FreeProxy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://162.248.225.122:80'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy = FreeProxy(country_id=['FR']).get(); proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.248.225.122\n"
     ]
    }
   ],
   "source": [
    "proxies = {'http': proxy} \n",
    "response = requests.get('http://httpbin.org/ip', proxies=proxies) \n",
    "print(response.json()['origin']) # our proxy !!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://162.248.225.131:80', 'http://162.248.225.224:80'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_list = [FreeProxy(country_id=['FR']).get() for x in range(3)]; set(proxy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we have now a different IP address, at least the server detect an other ip and not our public router IP üßôüèº‚Äç‚ôÇÔ∏è\n",
    "\n",
    "### Headers \n",
    "\n",
    "Now let's get deeper a little with our request header in order to fool our target with `User-Agent` (abbreviated as UA). A user agent is a string that a web browser sends to a web server identifying itself. This string contains details about the browser type, rendering engine, operating system, and sometimes device type. In web scraping, the user agent plays a crucial role for several reasons:\n",
    "\n",
    "- Browser Identification: The user agent tells the server what kind of browser is making the request. Different browsers may support different features or render web pages differently.\n",
    "- Device and OS Identification: The user agent can also indicate the operating system and the device (desktop, mobile, tablet, etc.), which can affect how web content is delivered.\n",
    "\n",
    "Let's see a basic example of the informations the target site will get if we use Python Requests or cURL without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-requests/2.31.0\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://httpbin.org/headers') \n",
    "print(response.json()['headers']['User-Agent']) \n",
    "# python-requests/2.25.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"headers\": {\r\n",
      "    \"Accept\": \"*/*\", \r\n",
      "    \"Host\": \"httpbin.org\", \r\n",
      "    \"User-Agent\": \"curl/7.64.1\", \r\n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-65786377-28e3d9436665d7ea0ab8992e\"\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl http://httpbin.org/headers # { ... \"User-Agent\": \"curl/7.74.0\" ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "#try a custom user-agent\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"} \n",
    "response = requests.get('http://httpbin.org/headers', headers=headers) \n",
    "print(response.json()['headers']['User-Agent']) # Mozilla/5.0 ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Linux; Android 10; SM-A505FN) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Mobile Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "#more user-agent, thanks chatgpt ü§ì\n",
    "import random\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (iPad; CPU OS 13_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/604.1.34 (KHTML, like Gecko) Edge/90.0.818.56',\n",
    "    'Mozilla/5.0 (Linux; Android 10; SM-A505FN) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Linux; Android 11; Pixel 3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0'\n",
    "]\n",
    "\n",
    "user_agent = random.choice(user_agents) \n",
    "headers = {'User-Agent': user_agent} \n",
    "response = requests.get('https://httpbin.org/headers', headers=headers) \n",
    "print(response.json()['headers']['User-Agent']) \n",
    "# Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a closer look to our request [here](http://httpbin.org/headers) you will see the entier header look like this :\n",
    "\n",
    "```bash\n",
    "{\n",
    "  \"headers\": {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate\", \n",
    "    \"Accept-Language\": \"fr-fr\", \n",
    "    \"Host\": \"httpbin.org\", \n",
    "    \"Upgrade-Insecure-Requests\": \"1\", \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6.1 Safari/605.1.15\", \n",
    "    \"X-Amzn-Trace-Id\": \"Root=1-6572fed4-5a7b863b4842def83f9030c4\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0\n"
     ]
    }
   ],
   "source": [
    "headers_list = [\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Ch-Ua\": \"\\\"Chromium\\\";v=\\\"92\\\", \\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"92\\\"\",\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    },\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0\"\n",
    "    },\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "    },\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-GB,en;q=0.5\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0\"\n",
    "    },\n",
    "    {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Host\": \"httpbin.org\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Safari/605.1.15\"\n",
    "    }\n",
    "]\n",
    "\n",
    "headers = random.choice(headers_list) \n",
    "response = requests.get('https://httpbin.org/headers', headers=headers) \n",
    "print(response.json()['headers']['User-Agent']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Selenium driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the environment variable for SSL certificate\n",
    "certi_path = \"/Users/mac/.pyenv/versions/3.7.0/lib/python3.7/site-packages/certifi/cacert.pem\"\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certi_path\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def initialize_driver(headers_list, proxy_list):\n",
    "    options = Options()\n",
    "    #select a random user-agent from the list\n",
    "    user_agent = random.choice(headers_list)[\"User-Agent\"]\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    #select a random proxy from the list\n",
    "    proxy = random.choice(proxy_list)\n",
    "    if proxy:\n",
    "        options.add_argument(f\"--proxy-server={proxy}\")\n",
    "    \n",
    "    #add some common options\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "\n",
    "    #initialize Chrome WebDriver with the specified options\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    #set implicit wait of 10sec\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    return driver\n",
    "\n",
    "# Example usage\n",
    "custom_driver = initialize_driver(headers_list, proxy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "import os\n",
    "# Set the environment variable for SSL certificate\n",
    "certi_path = \"/Users/mac/.pyenv/versions/3.7.0/lib/python3.7/site-packages/certifi/cacert.pem\"\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certi_path\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Initialize Chrome options\n",
    "options = Options()\n",
    "#options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
    "#disable the browser\n",
    "#options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "#options.add_argument(f'--proxy-server={proxy}')\n",
    "\n",
    "# Initialize Chrome WebDriver with the specified options\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the main page \n",
    "\n",
    "Our goal here is very simple: writing a python function called `MainPage` to get this url : `https://www.welcometothejungle.com/fr/jobs?page=1&configure%5Bfilters%5D=website.reference%3Awttj_fr&configure%5BhitsPerPage%5D=30&aroundQuery=France&refinementList%5Boffice.country_code%5D%5B%5D=FR&refinementList%5Bcontract_type_names.fr%5D%5B%5D=CDI&refinementList%5Bcontract_type_names.fr%5D%5B%5D=Stage&query=%22data%20analyst%22&range%5Bexperience_level_minimum%5D%5Bmin%5D=0&range%5Bexperience_level_minimum%5D%5Bmax%5D=1`\n",
    "\n",
    "and do a sleep of 3 seconds. \n",
    "\n",
    "```python \n",
    "def MainPage(driver, url):\n",
    "    '''Go the the first page and sleep(3)'''\n",
    "    #code here \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MainPage(driver, url):\n",
    "    '''Go the the first page'''\n",
    "    driver.get(url)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the number of offers per page\n",
    "\n",
    "\n",
    "Write a python function who return the number of job offer in a page \n",
    "\n",
    "```python\n",
    "def nbOffers(driver):\n",
    "    try:\n",
    "        #code here \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred in NB_OFFER:\", str(e))\n",
    "        return 0  # Or handle the exception as needed\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Example usage of the `nbOffers` function : \n",
    "\n",
    "```python \n",
    "url = f\"https://www.welcometothejungle.com/fr/jobs?page=1&configure%5Bfilters%5D=website.reference%3Awttj_fr&configure%5BhitsPerPage%5D=30&aroundQuery=France&refinementList%5Boffice.country_code%5D%5B%5D=FR&refinementList%5Bcontract_type_names.fr%5D%5B%5D=CDI&refinementList%5Bcontract_type_names.fr%5D%5B%5D=Stage&query=%22data%20analyst%22&range%5Bexperience_level_minimum%5D%5Bmin%5D=0&range%5Bexperience_level_minimum%5D%5Bmax%5D=1\"\n",
    "MainPage(driver, url)\n",
    "nb_offers = nbOffers(driver)\n",
    "```\n",
    "\n",
    "Then write a python function to get all the jobs post :\n",
    "\n",
    "```python \n",
    "def nbOffers_tot(driver):\n",
    "    try:\n",
    "        #code here\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred in NB_OFFER TOTAL:\", str(e))\n",
    "        return 0  # Or handle the exception as needed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbOffers(driver):\n",
    "    try:\n",
    "        # Wait for the job offer elements to be present on the page\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"li[class$='ais-Hits-list-item']\"))\n",
    "        )\n",
    "        \n",
    "        # Find all elements with the specified class\n",
    "        job_offers = driver.find_elements(By.CSS_SELECTOR, \"li[class$='ais-Hits-list-item']\")\n",
    "        #print(job_offers)\n",
    "        # The total number of job offers is the number of elements found\n",
    "        return len(job_offers)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred in NB_OFFER:\", str(e))\n",
    "        return 0  # Or handle the exception as needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nbOffers_tot(driver):\n",
    "    try:\n",
    "        # Wait for the div with the specified attribute to be available\n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='jobs-search-results-count']\"))\n",
    "        )\n",
    "        #return the text of the element\n",
    "        return int(element.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred in NB_OFFER TOTAL:\", str(e))\n",
    "        return 0  # Or handle the exception as needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numbers offers tot : 66 \n",
      "Number of offers per page : 30\n"
     ]
    }
   ],
   "source": [
    "#ouverture de la page et r√©cup√©ration du nombre d'offres\n",
    "url = f\"https://www.welcometothejungle.com/fr/jobs?page=1&configure%5Bfilters%5D=website.reference%3Awttj_fr&configure%5BhitsPerPage%5D=30&aroundQuery=France&refinementList%5Boffice.country_code%5D%5B%5D=FR&refinementList%5Bcontract_type_names.fr%5D%5B%5D=CDI&refinementList%5Bcontract_type_names.fr%5D%5B%5D=Stage&query=%22data%20analyst%22&range%5Bexperience_level_minimum%5D%5Bmin%5D=0&range%5Bexperience_level_minimum%5D%5Bmax%5D=1\"\n",
    "MainPage(driver, url)\n",
    "nb_offers = nbOffers(driver)\n",
    "nb_offerst = nbOffers_tot(driver)\n",
    "print(f\"\\nNumbers offers tot : {nb_offerst} \\nNumber of offers per page : {nb_offers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Click` and `getText` functions\n",
    "\n",
    "Write a python function who click on a given selenium element : \n",
    "\n",
    "```python \n",
    "\n",
    "def Click(driver, pos):\n",
    "    '''Click on the link'''\n",
    "    try:\n",
    "        #code here\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred in CLICK:\", str(e))\n",
    "        return 0  # Or handle the exception as needed\n",
    "```\n",
    "\n",
    "Then write a function who get the text with beatifulsoup of a job post, save it into a list and a txt file :\n",
    "```python\n",
    "def GetText(driver, jobs):\n",
    "    sleep(3)\n",
    "    #code here\n",
    "    try:\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error HTML PARSING: {e}\")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scroll(driver, i):\n",
    "    scroll_delta = int(250)\n",
    "    scroll_delta += 140*i\n",
    "    driver.execute_script(\"window.scrollBy(0, \"+ str(scroll_delta) + \")\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def Click(driver, pos):\n",
    "    '''Click on the link'''\n",
    "    try:\n",
    "        elements = driver.find_elements(By.CSS_SELECTOR, \"li[class$='ais-Hits-list-item']\")\n",
    "        target_element = elements[pos]\n",
    "        #click with javascript\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", target_element)\n",
    "        target_element.click()\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred in CLICK:\", str(e))\n",
    "        return 0  # Or handle the exception as needed\n",
    "\n",
    "\n",
    "def GetText(driver, jobs):\n",
    "    time.sleep(3)\n",
    "    #print(str(driver.current_url))\n",
    "    data = requests.get(driver.current_url)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    try:\n",
    "        info = soup.find(\"main\")\n",
    "        if info is not None:\n",
    "            info = info.find_all('script')\n",
    "            job = json.loads(str(info[0].string))\n",
    "            faq = json.loads(str(info[1].string))\n",
    "            job['FAQPage'] = faq['mainEntity']\n",
    "        else:\n",
    "            info = soup.find(\"div\").find_all('script')\n",
    "            job = json.loads(str(info[0].string))\n",
    "            faq = json.loads(str(info[1].string))\n",
    "            job['FAQPage'] = faq['mainEntity']\n",
    "\n",
    "        jobs.append(job)\n",
    "        f = open('wttj.txt', 'a')\n",
    "        f.write(str(job))\n",
    "        print(f'write job : {driver.current_url}')\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error HTML PARSING: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it into a loop üë®‚Äçüç≥üë©‚Äçüç≥\n",
    "\n",
    "Write a simple loop over page in order to put all the jobs into a list named `jobs` you can add a `break` statement for the debuging part, it can be long ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write job : https://www.welcometothejungle.com/fr/companies/matera/jobs/data-analyst-stage-de-6-mois_paris?q=8a940671e03319920400c3a073bdcf37&o=2228430\n",
      "\n",
      "2/66write job : https://www.welcometothejungle.com/fr/companies/pwc/jobs/data-analyst-deals-m-a-cdi-h-f_paris_PF_GPPgd6A?q=b9ec9f46e728f4465929db3fb001b7ec&o=2262089\n",
      "\n",
      "3/66"
     ]
    }
   ],
   "source": [
    "# boucle de scraping\n",
    "counter = 1\n",
    "jobs = []\n",
    "for i in range (1, (nb_offerst // 30) + 1):\n",
    "    url = f\"https://www.welcometothejungle.com/fr/jobs?page={i}&configure%5Bfilters%5D=website.reference%3Awttj_fr&configure%5BhitsPerPage%5D=30&aroundQuery=France&refinementList%5Boffice.country_code%5D%5B%5D=FR&refinementList%5Bcontract_type_names.fr%5D%5B%5D=CDI&refinementList%5Bcontract_type_names.fr%5D%5B%5D=Stage&query=%22data%20analyst%22&range%5Bexperience_level_minimum%5D%5Bmin%5D=0&range%5Bexperience_level_minimum%5D%5Bmax%5D=1\"\n",
    "    for j in range(0,30):\n",
    "        MainPage(driver, url)\n",
    "        scroll(driver, j)\n",
    "        Click(driver, j)\n",
    "        GetText(driver, jobs)\n",
    "        counter += 1\n",
    "        print(f\"\\n{counter}/{int(nb_offerst)}\", end='')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean it and insert into PostgresSQL database\n",
    "\n",
    "Here our mission is simple clean the data as least a little in order to insert them into a PostgresSQL database. \n",
    "\n",
    "0. Transform our job list into a pandas Dataframe\n",
    "1. Clean the text inside the `description` column\n",
    "- Write a function called `extract_salary_info()` who split the `baseSalary` columns into `['minSalary', 'maxSalary', 'currency', 'salaryUnit']` \n",
    "- Extract the `name` variable inside the `hiringOrganization` column\n",
    "- Extract the `addressLocality` variable inside the `JobLocation` column\n",
    "- Drop the columns `['@context','baseSalary','educationRequirements','experienceRequirements','FAQPage']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.DataFrame(jobs); jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from HTML\n",
    "def extract_text_from_html(html_content):\n",
    "    if pd.isna(html_content):\n",
    "        return None\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "# Apply function to description column\n",
    "jobs_df['description'] = jobs_df['description'].apply(extract_text_from_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary_info(salary_data):\n",
    "    if pd.isna(salary_data):\n",
    "        return [None, None, None, None]\n",
    "    currency = salary_data.get('currency', None)\n",
    "    value = salary_data.get('value', {})\n",
    "    min_value = value.get('minValue', None)\n",
    "    max_value = value.get('maxValue', None)\n",
    "    unit_text = value.get('unitText', None)\n",
    "    return [min_value, max_value, currency, unit_text]\n",
    "\n",
    "# Apply the function to baseSalary column and create new columns\n",
    "jobs_df[['minSalary', 'maxSalary', 'currency', 'salaryUnit']] = jobs_df['baseSalary'].apply(lambda x: pd.Series(extract_salary_info(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases insertion\n",
    "\n",
    "Our goal in this part is to insert our result data into a postgres database üòé\n",
    "\n",
    "We will use docker to run our postgres database in a simple way  with this command :\n",
    "\n",
    "```bash\n",
    "docker run --name posttest -d -p 5432:5432 -e POSTGRES_PASSWORD=fred postgres:alpine\n",
    "```\n",
    "\n",
    "You mission is simple : write data to the database in a `job_table` table !\n",
    "\n",
    "You can use this sample code to connect your database :\n",
    "```python \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database credentials\n",
    "user = 'postgres'\n",
    "password = 'fred'\n",
    "host = '0.0.0.0'  # or the IP if your PostgreSQL server is running elsewhere\n",
    "port = '5432'       # default port for PostgreSQL used by our docker above\n",
    "db = 'postgres'\n",
    "\n",
    "# Create the connection\n",
    "engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "```\n",
    "\n",
    "Then write a little script who connect to the database and list all the tables inside then perform a verification query (e.g., selecting the first 5 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install psycopg2 sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --name posttest -d -p 5432:5432 -e POSTGRES_PASSWORD=fred postgres:alpine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database credentials\n",
    "user = 'postgres'\n",
    "password = 'fred'\n",
    "host = '0.0.0.0'  # or the IP if your PostgreSQL server is running elsewhere\n",
    "port = '5432'       # default port for PostgreSQL\n",
    "db = 'postgres'\n",
    "\n",
    "# Create the connection\n",
    "engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df['hiringOrganization'] = jobs_df['hiringOrganization'].apply(lambda x: x['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df['jobLocation'] = jobs_df['jobLocation'].apply(lambda x: x[0]['address']['addressLocality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = jobs_df.drop(['@context','baseSalary','educationRequirements','experienceRequirements','FAQPage'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to the database in `job_table` table\n",
    "jobs_df.to_sql('job_table', engine, if_exists='replace', index=False)  # Choose 'append' if you don't want to overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "#connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(dbname='postgres', user=user, password=password, host=host, port=port)\n",
    "\n",
    "#create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#list all tables in the database\n",
    "cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table)\n",
    "\n",
    "#check if job_table is created\n",
    "if ('job_table',) in tables:\n",
    "    print(\"\\nTable 'job_table' exists.\")\n",
    "else:\n",
    "    print(\"\\nTable 'job_table' does not exist.\")\n",
    "\n",
    "#perform a verification query (e.g., selecting the first 5 rows)\n",
    "query = \"SELECT * FROM job_table LIMIT 5\"\n",
    "df = pd.read_sql(query, conn)\n",
    "print(\"\\nFirst 5 rows of 'job_table':\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB\n",
    "\n",
    "Same mission with mongo :\n",
    "\n",
    "```\n",
    "docker run -d --name example-mongo -p 27017:27017 mongo\n",
    "```\n",
    "\n",
    "Connect the mongo database and do a dummy query like *find the number of documents where the currency is 'EUR'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d --name example-mongo -p 27017:27017 mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "#connect to the MongoDB server (default is localhost on port 27017)\n",
    "client = MongoClient('0.0.0.0', 27017)\n",
    "\n",
    "#access the database (create it if it doesn't exist)\n",
    "db = client['job_database']\n",
    "\n",
    "#access the collection (similar to a table in relational databases)\n",
    "collection = db['job_collection']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert DataFrame to dictionary format and insert into our MongoDB database\n",
    "collection.insert_many(jobs_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the number of documents where the currency is 'EUR'\n",
    "count = 0\n",
    "for job in collection.find({\"currency\": \"EUR\"}):\n",
    "    count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
